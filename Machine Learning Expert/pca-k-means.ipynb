{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### 1: Principal Components Analysis  \n",
    "This may be helpful:\n",
    "Jake VanderPlas' Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "You can assume you have a training set 𝑋 with 𝑁 examples, each one a feature vector with 𝐹 features.\n",
    "\n",
    "####  1a: True/False, providing explanation for each one\n",
    "\n",
    "* 1a(i): You compute principal components by finding eigenvectors of the dataset's feature matrix 𝑋. - **True. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k≤d).Construct the projection matrix W from the selected k eigenvectors.Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.**\n",
    "\n",
    "\n",
    "* 1a(ii): To select the number of components 𝐾 to use, you can find the value of 𝐾 that minimizes reconstruction error on the training set. This choice will help manage accuracy and model complexity.**False. We select K based on whole dataset after performing PCA and not on training dataset.K is selected by plotting number of components against cummulative explained ratio.This curve shows how much of original data is covered in number of selected components K.**\n",
    "\n",
    "\n",
    "* 1a(iii): If we already have fit a PCA model with 𝐾=10 components, fitting a PCA model with 𝐾=9 components for the same dataset is easy.**False. We need to create another model using K=10. When we create model using scikit-learn's PCA class we provide number of components as input to it. We then train model using this K=10. If we need to projection with K=9 feature then we can not use same trained PC of K=10 features. We need to define new PCA with K=9 and train it.**\n",
    "\n",
    "#### 1b: You had a dataset for a medical application, with many measurements describing the patient's height, weight, income, daily food consumption, and many other attributes.\n",
    "\n",
    "* 1b(i): Would the PCA components learned be the same if you used feet or centimeters to measure height? Why or why not? **PCA components learned will be different if height is measured in different units of measurements. Because covariance matrix learned from dataset measuring height in feet and centimeters will be different which will results in different in eigenvalues & eigenvectors and total different projection matrix. If PCA is based on correlation matrix then difference in units measurement will not affect.To avoid this kind of problems, its generally good practice to standardize data using scalers like StandardScaler, RobustScaler, MinMaxScaler etc.**\n",
    "\n",
    "\n",
    "* 1b(ii): Before applying PCA to this dataset, what (if any) preprocessing would you recommend and why? **Yes. PCA is affected by feature in data measured in different units of measurements. This can affect overall results on data transformed. To avoid this kind of problems, its generally good practice to standardize data using scalers like StandardScaler, RobustScaler, MinMaxScaler etc.**\n",
    "\n",
    "\n",
    "#### 1c: Stella has a training dataset , which has 𝑁 example feature vectors 𝑥𝑛 each with 𝐹 features. She applies PCA to her training set to learn a matrix 𝑊 of shape (𝐾,𝐹), where each row represents the basis vector for component 𝑘.She would like to now project her test dataset  using the same components 𝑊. She remembers that she needs to center her data, so she applies the following 3 steps to project each test vector x′ to a 𝐾-dimensional vector z′:\n",
    "$$\n",
    "m = \\frac{1}{T} \\sum_{t=1}^T x'_t\n",
    "\\\\\n",
    "\\tilde{x}'_t = x'_t - m\n",
    "\\\\\n",
    "z_t = W \\tilde{x}'_t\n",
    "$$\n",
    "\n",
    "Is this correct? Why??? If not, explain what Stella should do differently.\n",
    "\n",
    "**It's not correct. Centering data requires calculation of standard deviation as well and formula for centering is subtracting mean and dividing by standard deviation. Please find below correct formula.**\n",
    "\n",
    "$$\n",
    "m = \\frac{1}{T} \\sum_{t=1}^T x'_t\n",
    "\\\\\n",
    "\\mu = \\sqrt{\\sum_{i=1}^T (x_i' - m)^2}.\n",
    "\\\\\n",
    "\\tilde{x}'_t = \\dfrac {(x'_t - m)} \\mu\n",
    "\\\\\n",
    "z_t = W \\tilde{x}'_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "### 2: K-Means Clustering\n",
    "Consider the k-means clustering algorithm, this maybe helpful?\n",
    "Jake VanderPlas' Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "You can assume you have a training set 𝑋 with 𝑁 examples, each one a feature vector with 𝐹 features.\n",
    "#### 2a: True/False questions with one sentence of explanation for each one\n",
    "\n",
    "* 2a(i): you always get the same clustering of dataset 𝑋 when applying K-means with 𝐾=1, no matter how you initialize the cluster centroids 𝜇 - **True. As number of clusters are just 1, it won't affect where we initialize it.**\n",
    "\n",
    "\n",
    "* 2a(ii): you always get the same clustering of dataset 𝑋 when applying K-means with 𝐾=2, no matter how you initialize the cluster centroids 𝜇 - **False. If we initialize KMeans with different random_state then clustering results will be different. Based on random_state KMeans intializes initial clustering center. KMeans is vulnerable to different initialization of cluster centroids.**\n",
    "\n",
    "\n",
    "* 2a(iii): The only way to find the cluster centroids 𝜇 that minimize the K-means cost function (minimize sum of distances to nearest centroid) is to apply the K-means algorithm, alternatively updating assignments and cluster centroid locations. - **True. It starts with randomly guessing cluster centers. Assign points to nearest cluster based on Euclidean distance. After all points are assigned then update cluster centers to mean moving it to new location. Repeast process until convergence where cluster centers are not moving anymore.**\n",
    "\n",
    "\n",
    "* 2a(iv): The K-means cost function requires computing the Euclidean distance from each example 𝑥𝑛 to its nearest cluster centroid 𝜇𝑘. Because the Euclidean distance requires a square root operation (e.g. np.sqrt or np.pow(___, 0.5)), no implementation of the K-means algorithm can be correct unless a \"sqrt\" operation is performed when computing distances between examples and clusters. - **False. It's not necessary to perform square root on distance. We can minimize these squared distance instead of squared root one. If we don't compute squared root then we can avoid that step and algorithm will run bit faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b: Suppose you are given a dataset 𝑋 with 𝑁 examples, as well as a group of 𝐾=5 cluster locations  fit by the K-means algorithm to this dataset. You know 𝑁>5. Describe how you could initialize K-means using 𝐾=6 clusters to obtain a better cost than the 𝐾=5 solution.\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(\n",
    "        n_clusters=6, random_state=42, init='k-means++', n_init=10, algorithm='auto')\n",
    "\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "#### 2c: Suppose you are using sklearn's implementation of K-means to fit 10 clusters to a dataset.\n",
    "You start with code like this:\n",
    "    \n",
    "    kmeans = sklearn.cluster.KMeans(\n",
    "        n_clusters=10, random_state=42, init='random', n_init=1, algorithm='full')\n",
    "\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "List at least two changes you might make to these keyword arguments to improve the quality of your learned \n",
    "clusters (as measured by the K-means cost function).\n",
    "\n",
    "**Below are changes suggested which will improve quality of learned model.**\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(\n",
    "        n_clusters=10, random_state=42, init='k-means++', n_init=10, algorithm='full')\n",
    "\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    We have made 2 changes. n_init changed to '10' from '1' and init changed to 'k-means++' from 'random'.\n",
    "    \n",
    "    n_init is number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    \n",
    "    'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
    "    \n",
    "    \n",
    "#### 2d: Consider the two steps of the k-means algorithm, assign_examples_to_clusters and update_cluster_locations. Given docstrings for these steps below describing input and output, so you can be sure what happens in each step.\n",
    "\n",
    "* 2d(i): What is the big-O runtime of assign_examples_to_clusters? And explain why? Express in terms of 𝑁,𝐾,𝑎𝑛𝑑 𝐹. - **It'll take O(N) running time.As it loops through each examples and take time to loop through all examples.**\n",
    "\n",
    "\n",
    "* 2d(ii): What is the big-O runtime of update_cluster_locations? And explain why? Express in terms of 𝑁,𝐾,𝑎𝑛𝑑 𝐹. - **It'll take O(K*F) running time. As it loop through all m_KF members and updates values. m_KF is of size KxF.**\n",
    "    \n",
    "        def assign_examples_to_clusters(x_NF, m_KF):\n",
    "        ''' Assign each training feature vector to closest of K centroids\n",
    "\n",
    "        Returned assignments z_N will, for each example n,\n",
    "        provide the index of the row of m_KF that is closest to vector x_NF[n]\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_NF : 2D array, size n_examples x n_features (N x F)\n",
    "            Observed data feature vectors, one for each example\n",
    "        m_KF : 2D array, size n_clusters x n_features (K x F)\n",
    "            Centroid location vectors, one for each cluster\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        z_N : 1D array, size N\n",
    "            Integer indicator of which cluster each example is assigned to.\n",
    "            Example n is assigned to cluster k if z_N[n] = k\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "        def update_cluster_locations(x_NF, z_N):\n",
    "        ''' Update the locations of each cluster\n",
    "\n",
    "        Returned centroid locations will minimize the distance between\n",
    "        each cluster k's vector m_KF[k] and its assigned data x_NF[z_N == k]\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_NF : 2D array, size n_examples x n_features (N x F)\n",
    "            Observed data feature vectors, one for each example\n",
    "        z_N : 1D array, size N\n",
    "            Integer indicator of which cluster each example is assigned to.\n",
    "            Example n is assigned to cluster k if z_N[n] = k\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        m_KF : 2D array, size n_clusters x n_features (K x F)\n",
    "            Centroid location vectors, one for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_biclusters, make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 20), (100,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = make_classification()\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans1 = KMeans(1, random_state=0)\n",
    "kmeans1.fit(X, Y)\n",
    "Y1 = kmeans1.predict(X)\n",
    "\n",
    "kmeans2 = KMeans(1, random_state=123)\n",
    "kmeans2.fit(X, Y)\n",
    "Y2 = kmeans2.predict(X)\n",
    "\n",
    "np.all(Y1 == Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans1 = KMeans(2, random_state=0)\n",
    "kmeans1.fit(X, Y)\n",
    "Y1 = kmeans1.predict(X)\n",
    "#print(Y1)\n",
    "kmeans2 = KMeans(2, random_state=123)\n",
    "kmeans2.fit(X, Y)\n",
    "Y2 = kmeans2.predict(X)\n",
    "#print(Y2)\n",
    "np.all(Y1 == Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=8, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
